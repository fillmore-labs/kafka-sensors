= Kafka Serialization Playground
:Author:    Oliver Eikemeier
:Email:     <eikemeier@fillmore-labs.com>
:Date:      2021-10
:Revision:  v0.1
:toc: macro

image:https://badge.buildkite.com/4971a2c4dd08b27b3a39618d744d15ebac5a07c1892102c0b6.svg?branch=main[title="Buildkite build status",link=https://buildkite.com/fillmore-labs/kafka-sensors]
image:https://codecov.io/gh/fillmore-labs/kafka-sensors/branch/main/graph/badge.svg?token=BHQ06364X7[title="Codecov test coverage",link=https://codecov.io/gh/fillmore-labs/kafka-sensors]
image:https://api.codeclimate.com/v1/badges/787f6eeda9f0fba0355c/maintainability[title="Code Climate maintainability",link=https://codeclimate.com/github/fillmore-labs/kafka-sensors/maintainability]
image:https://img.shields.io/github/license/fillmore-labs/kafka-sensors[title="License",link=https://github.com/fillmore-labs/kafka-sensors/blob/main/LICENSE]

toc::[]

== Purpose

This source demonstrates how to process a stream of sensor data using
https://kafka.apache.org/documentation/streams/[Kafka Streams].

The sensors produce a stream of records, including sensor ID, a timestamp and the current state (on
or off). The desired result is a stream of records enriched with the duration the sensor has been in
this state.

=== Example

For example, a stream

.Sensor Data
|===
|Name|Timestamp|State

|Sensor 1
|1984-01-22T15:45:00Z
|off

|Sensor 1
|1984-01-22T15:45:10Z
|off

|Sensor 1
|1984-01-22T15:45:30Z
|on

|Sensor 1
|1984-01-22T15:46:30Z
|off
|===

should produce

.Enriched Data
|===
|Name|Timestamp|State|Duration

|Sensor 1
|1984-01-22T15:45:00Z
|off
|10s

|Sensor 1
|1984-01-22T15:45:00Z
|off
|30s

|Sensor 1
|1984-01-22T15:45:30Z
|on
|60s
|===

Which tells us that “Sensor 1” was “off” from 15:45:00 for 30 seconds and “on” from 15:45:30 for 60
seconds.

Note that the second “off” reading produced an intermediate result.

=== Design decisions

Duplicate readings of the same state generate intermediate results, and delayed readings (timestamps
preceding previously seen values) are treated as errors.

These are deliberate choices and can easily be changed.

=== Implementation of Business Logic

Care has been taken to keep the business logic independent of implementation details like
serialization formats.

The data model is in the link:src/main/java/com/fillmore_labs/kafka/sensors/model[model directory], the
business logic in link:src/main/java/com/fillmore_labs/kafka/sensors/logic[logic].

The link:src/test/java/com/fillmore_labs/kafka/sensors/topology[tests] test the topology with eight
different formats, https://developers.google.com/protocol-buffers/[protocol buffers],
https://json.org[JSON], https://avro.apache.org/docs/current/[Apache Avro], the
https://docs.confluent.io/platform/current/schema-registry/[Confluent variants] of them,
https://thrift.apache.org[Apache Thrift] and https://amzn.github.io/ion-docs/[Amazon Ion].
Different, random combinations of input format, result format, and format for the state store are
tested.

While this abstraction might not be necessary in practice, it demonstrates two important design
considerations:

* The business logic should only depend on a data model, not capabilities of the serialization
mechanism.

We can simply use
https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/Duration.html#between(java.time.temporal.Temporal,java.time.temporal.Temporal)[`Duration::between`],
which is a simple call and easy to understand and test, instead of cluttering our logic with
conversions and unnecessary error-prone calculations.

* The choice of (de-)serializers should depend on the requirements, not on what is just at hand.

While internal processing pipelines tend (but don't have) to use one serialization mechanism, it
perfectly valid and a good design decision to use different mechanisms for parts interfacing with
external components.

Since the business logic is independent of the serialization mechanism, changing it is simple and
usually does not require retesting.

By refactoring the business logic to depend only on an abstract store, we speed up testing by a
factor of seven
([source,shell]`bazel test //src/test/java/com/fillmore_labs/kafka/sensors/logic:all` vs.
`bazel test //src/test/java/com/fillmore_labs/kafka/sensors/topology:all`), which demonstrates a potential
for improvement in development speed and testability.

== Running

=== Prerequisites

You need https://github.com/bazelbuild/bazelisk[Bazelisk] installed, see also
https://docs.bazel.build/versions/main/install-bazelisk.html[Installing Bazel using Bazelisk].

==== macOS

Using https://brew.sh[HomeBrew] enter

[source,shell]
brew install bazelbuild/tap/bazelisk

==== Windows

Using https://chocolatey.org[Chocolatey] enter

[source,shell]
choco install bazelisk

Enable developer mode:

. Open Windows settings
. Go to “Update & security”, then “For developers”
. Under “Developer Mode” section enable “Install apps from any source, including loose files”.

or run with administrator privileges.

=== Tests

To run all tests, use

[source,shell]
bazel test //src/test/...

To run a single test, use

[source,shell]
bazel test //src/test/java/com/fillmore_labs/kafka/sensors/topology:all

The tests run with an embedded Kafka and mock schema registry, when necessary.

=== Main App

The main app needs Kafka running at `localhost`, port 9092 (see
link:conf/application.yaml[application.yaml]). There is a script doing that:

[source,shell]
scripts/kafka-server.sh

When Kafka has finished starting, create the topics in a different terminal:

[source,shell]
scripts/kafka-topics.sh

Now start the main app:

[source,shell]
bazel run //:kafka-sensors

Open another terminal to watch the results:

[source,shell]
scripts/kafka-consume.sh

Publish sensor values:

[source,shell]
scripts/kafka-produce.sh

=== Benchmark

Run the https://openjdk.java.net/projects/code-tools/jmh/[JMH] microbenchmarks with

[source,shell]
bazel run //:benchmark

[source,shell]
bazel run //:benchmark -- -p "format=proto" "Bench\\.deserialize"

[source,shell]
bazel run //:benchmark -- -p "format=proto" "Bench\\.deserialize" \
    -prof "async:output=flamegraph;direction=forward"
open "$(bazel info bazel-bin)/src/main/java/com/fillmore_labs/kafka/sensors/benchmark/benchmark.runfiles/com_fillmore_labs_kafka_sensors/com.fillmore_labs.kafka.sensors.benchmark.Bench.deserialize-AverageTime-format-proto/flame-cpu-forward.html"

== Notes
=== Mapping

As noted in <<Implementation of Business Logic>> the business login is independent of the
serialization, in the spirit of hexagonal architecture. This of course requires some mapping,
where we mostly use https://mapstruct.org[MapStruct] for. This necessitates some limitations in
data model naming conventions. MapStruct uses a fixed und quite unflexible accessor naming strategy,
so you can't really decide that protocol buffers should have one convention but Immutables another.
Especially for Immutables we are forced to use JavaBeans-style naming convention, although this is
not a JEE application.
